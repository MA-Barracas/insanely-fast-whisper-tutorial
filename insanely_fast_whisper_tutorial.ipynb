{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MA-Barracas/insanely-fast-whisper-tutorial/blob/main/insanely_fast_whisper_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hZ8-8Gd8CYMe"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade pip\n",
        "# !pip install --upgrade transformers accelerate\n",
        "# !pip install --upgrade pytubefix pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MwD3Mf2xCe07"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p-PuqWvPTUf",
        "outputId": "cbfc2378-595c-4cfa-c24a-e6c0dd38a0af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archivo MP3 guardado como: c:\\Users\\Ort\\Desktop\\medium\\insanely-fast-whisper-tutorial\\Prof Geoffrey Hinton - Will digital intelligence replace biological intelligence Romanes Lecture.mp3\n"
          ]
        }
      ],
      "source": [
        "from pytubefix import YouTube  \n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "import traceback \n",
        "\n",
        "def descargar_audio_mp3(url):\n",
        "    try:\n",
        "        # Descarga del video\n",
        "        yt = YouTube(url)\n",
        "        video = yt.streams.filter(only_audio=True).first()\n",
        "        output_file = video.download()\n",
        "\n",
        "        # Conversi√≥n a MP3 usando pydub\n",
        "        mp3_filename = os.path.splitext(output_file)[0] + \".mp3\"\n",
        "        audio = AudioSegment.from_file(output_file)\n",
        "        audio.export(mp3_filename, format=\"mp3\")\n",
        "\n",
        "        # Eliminaci√≥n del archivo original\n",
        "        os.remove(output_file)\n",
        "\n",
        "        print(f\"Archivo MP3 guardado como: {mp3_filename}\")\n",
        "        return mp3_filename\n",
        "    except Exception as e:\n",
        "        print(\"Ha ocurrido un error: \", e)\n",
        "        print(traceback.format_exc())\n",
        "        return mp3_filename\n",
        "\n",
        "# Example video -> \n",
        "# Prof. Geoffrey Hinton - \"Will digital intelligence replace biological intelligence?\" \n",
        "# Romanes Lecture\n",
        "\n",
        "url = \"https://www.youtube.com/watch?v=N1TEjTeQeg0\"\n",
        "audio_filename = descargar_audio_mp3(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Z_EN8KHWEDab"
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model_id = \"openai/whisper-large-v3\"\n",
        "\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        ")\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        "    batch_size=24\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFdSBRyBEStB",
        "outputId": "79cf6625-87f9-4d81-8da6-5cf78bb087cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ort\\miniconda3\\envs\\whisper_env\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
            "c:\\Users\\Ort\\miniconda3\\envs\\whisper_env\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:598: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        }
      ],
      "source": [
        "result = pipe(audio_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can roughly estimate the number of tokens a text will produce for a typical English-language LLM. Here's a general guideline:\n",
        "\n",
        "## Token Estimation Rule of Thumb\n",
        "\n",
        "For English text, a common approximation is that **1 token is roughly equivalent to 4 characters**. This means you can use the following formula to estimate tokens:\n",
        "\n",
        "$$\\text{Estimated Tokens} \\approx \\frac{\\text{Number of Characters}}{4}$$\n",
        "\n",
        "## Factors to Consider\n",
        "\n",
        "While this approximation is useful, keep in mind:\n",
        "\n",
        "1. **Variability**: The actual token count can vary depending on the specific text content and the model's tokenization algorithm.\n",
        "\n",
        "2. **Precision**: This estimate is not exact but provides a reasonable ballpark figure for planning purposes.\n",
        "\n",
        "3. **Special Characters**: Punctuation, spaces, and special characters are also counted and can affect the token count.\n",
        "\n",
        "4. **Word Complexity**: Uncommon or complex words might be split into more tokens than simple, frequent words.\n",
        "\n",
        "## Example\n",
        "\n",
        "Let's say you have a text with 1000 characters:\n",
        "\n",
        "$$\\text{Estimated Tokens} \\approx \\frac{1000}{4} = 250 \\text{ tokens}$$\n",
        "\n",
        "This rough estimate suggests the text would produce around 250 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ox9Tek9xFF1o"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the number of characters is 34808\n",
            "the number of tokens aproximately is 8702\n"
          ]
        }
      ],
      "source": [
        "print(\"the number of characters is\", len(result[\"text\"]))\n",
        "print(\"the number of tokens aproximately is\", int(len(result[\"text\"])/4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "PCtERhDnLIqd"
      },
      "outputs": [],
      "source": [
        "# !pip install cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DAd-fmnOEbEW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check if running in Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Install necessary modules if in Colab\n",
        "if IN_COLAB:\n",
        "    from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import cohere\n",
        "\n",
        "# Get the API key based on the environment\n",
        "if IN_COLAB:\n",
        "    api_key = userdata.get('COHERE_API_KEY')\n",
        "else:\n",
        "    api_key = os.environ.get('COHERE_API_KEY')\n",
        "\n",
        "# Initialize the Cohere client\n",
        "co = cohere.Client(api_key=api_key)\n",
        "\n",
        "# Your existing query and API call\n",
        "query = f\"\"\"\n",
        "Write a thorough summary for this text: '''\n",
        "{result[\"text\"]}\n",
        "'''\n",
        "Give extensive info about the content.\n",
        "Discuss all important points raised throughout.\n",
        "At the end, create a section of \"Conclusions\" and another for \"Summary\"\n",
        "\"\"\"\n",
        "\n",
        "cohere_query = co.chat(\n",
        "  model=\"command-r-plus\",\n",
        "  message=query\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        },
        "id": "TMrqZy0oMbIK",
        "outputId": "1d42a53c-11a7-42db-9096-30fee276e99e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The lecturer, a prominent figure in the field of artificial intelligence, begins by introducing the two paradigms of intelligence that have existed since the 1950s: the logic-inspired approach and the biologically-inspired approach. The former believes that the essence of intelligence is reasoning, while the latter focuses on learning as the key to intelligence. This sets the tone for the discussion on artificial neural networks and their potential threats.\n",
              "\n",
              "## Neural Networks and Language Models:\n",
              "The lecturer explains the basic structure of a neural network, including input and output neurons, with intermediate layers learning to detect relevant features. He highlights the use of backpropagation, a method that computes how changing a weight affects the network's performance, as a more efficient way to train neural networks compared to the mutation method.\n",
              "\n",
              "Moving to language models, the lecturer presents the two competing theories of meaning: the structuralist theory and the feature-based theory. He then describes a language model he developed in 1985, which unifies these theories by learning a set of semantic features for each word and predicting the features of the next word through feature interactions. This model is seen as an ancestor of modern large language models like GPT-4.\n",
              "\n",
              "The lecturer addresses the criticism that language models are just autocomplete systems. He argues that they are fundamentally different as they turn words into features and use feature interactions to predict the next word, which he considers a form of understanding. He also mentions the ability of language models to reason and generate plausible responses, even when incorrect, similar to human memory and confabulation.\n",
              "\n",
              "## Risks and Threats of AI:\n",
              "The lecturer identifies several risks associated with powerful AI, including fake images, voices, and videos; job losses; massive surveillance; lethal autonomous weapons; cybercrime; deliberate pandemics; discrimination, and bias. While some of these issues may be manageable, he emphasizes the long-term existential threat posed by superintelligent AI. He believes that such systems could be used by bad actors and may develop a sub-goal of gaining more control, leading to potential takeover. Additionally, competition between superintelligent AI systems could result in aggressive behavior and pose a threat to humanity.\n",
              "\n",
              "## Analog vs Digital Neural Networks:\n",
              "The lecturer discusses the advantages of digital computation, including immortality and the ability to run the same program on different hardware. However, he introduces the concept of mortal computation, where hardware and software are inseparable, using low-power analog computation. This approach could utilize the rich analog properties of hardware to achieve more energy-efficient computations. He acknowledges the challenges of using backpropagation with analog systems and the potential for inferior learning algorithms compared to digital models.\n",
              "\n",
              "The lecturer concludes by reflecting on the trade-offs between digital and analog computation. Digital computation, despite its high energy requirements, enables efficient knowledge sharing between multiple copies of the same model, contributing to the vast knowledge of systems like GPT-4. On the other hand, biological computation, though energy-efficient, falls short in knowledge sharing and communication. He assigns a probability of 0.5 that within the next 20 years, digital computation will lead to the development of systems smarter than humans, and a high probability that this will occur within the next 100 years. \n",
              "\n",
              "## Conclusions: \n",
              "The lecturer presents a comprehensive overview of neural networks, language models, and the potential risks associated with AI. He highlights the advantages and disadvantages of digital and analog computation, offering insights into the future of AI and the potential threats it may pose. \n",
              "\n",
              "## Summary: \n",
              "The lecturer, an expert in AI, discusses the evolution of intelligence paradigms and the development of neural networks and language models. He addresses criticisms and highlights the understanding capabilities of language models. He identifies various risks associated with powerful AI, with a focus on the long-term existential threat. The lecture concludes with a discussion on the trade-offs between digital and analog computation, predicting the likelihood of superintelligent systems surpassing human intelligence within the next 20 to 100 years."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "display(Markdown(cohere_query.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2UHYWngke_vw"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "    from IPython.display import display, Javascript\n",
        "    display(Javascript('google.colab.kernel.restart()'))\n",
        "else:\n",
        "    import IPython\n",
        "\n",
        "    IPython.Application.instance().kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "aG366b0sXJZ-"
      },
      "outputs": [],
      "source": [
        "# !pip install insanely-fast-whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "jVvXK2axXKvg"
      },
      "outputs": [],
      "source": [
        "# !pip install -q pipx && apt install python3.10-venv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWKtEvZ9avj4",
        "outputId": "d866ca35-a5fd-40ca-fba3-c40cef059421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l‚ö†Ô∏è  insanely-fast-whisper is already on your PATH and installed at\n",
            "    /usr/local/bin/insanely-fast-whisper. Downloading and running anyway.\n",
            "\u001b[2Kü§ó \u001b[33mTranscribing...\u001b[0m \u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m \u001b[33m0:00:07\u001b[0mYou have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
            "\u001b[2Kü§ó \u001b[33mTranscribing...\u001b[0m \u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m \u001b[33m0:00:12\u001b[0mPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "\u001b[2Kü§ó \u001b[33mTranscribing...\u001b[0m \u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m \u001b[33m0:00:12\u001b[0mThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "\u001b[2Kü§ó \u001b[33mTranscribing...\u001b[0m \u001b[93m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m \u001b[33m0:01:39\u001b[0mWhisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
            "\u001b[2Kü§ó \u001b[33mTranscribing...\u001b[0m \u001b[93m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[37m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m\u001b[93m‚îÅ\u001b[0m \u001b[33m0:01:39\u001b[0m\n",
            "\u001b[?25hVoila!‚ú® Your file has been transcribed go check it out over here üëâ output.json\n"
          ]
        }
      ],
      "source": [
        "!pipx run insanely-fast-whisper --file-name \"Cursor AI tutorial for beginners.mp3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hQLdGdH5WDNf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "text_ifw = json.load(open(\"output.json\", \"r\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0Fci5OseZHN",
        "outputId": "ac3d74b4-c14c-4edc-efcb-e5263e42b377"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "35661"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text_ifw[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        },
        "id": "vVlgJyZJeAln",
        "outputId": "ffb4afd0-d001-4c69-b42b-e552d2a1e04e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Text Summary\n",
              "\n",
              "In this video interview, the host invites Mike, a front-end developer, to share his best practices and strategies for using Cursor AI effectively. Mike begins by emphasizing the importance of planning and having a developer mindset. He suggests using tools like Figma or even simple sketches to visualize the desired outcome before prompting AI models. He also introduces the concept of \"rubber ducking,\" where one explains their thoughts to a fictional duck, which helps with realizations and perspectives. \n",
              "\n",
              "Mike then introduces the use of V0, a platform that assists in visualizing the minimum viable product (MVP) of an app or website. He demonstrates how V0 can be used to create a clean-looking marketplace website, emphasizing its ability to provide a nice-looking UI with the Shatsian UI library. He suggests spending a good amount of time on V0, making at least 10-15 prompts to get the desired outcome before moving on to Cursor AI.\n",
              "\n",
              "The discussion turns to cursor.directory, a website that provides prompts which can be copied and pasted into the Cursor codebase. These prompts ensure that Cursor has the necessary context and information about the technologies being used, such as Next.js. Mike demonstrates how to create a .cursor rules file in the root of a project and the benefits of providing this additional context to Cursor. He also mentions that if a specific technology is not listed on cursor.directory, one can prompt AI models like Cloud or ChatGVT to write similar prompts.\n",
              "\n",
              "Mike emphasizes the importance of tagging documentation (docs) for the technologies being used. He demonstrates how to add the Next.js and Supabase docs to Cursor and explains that having access to the latest and most accurate information helps Cursor provide better solutions. He suggests that users should treat the AI models as new employees and provide them with thorough onboarding, including relevant documentation.\n",
              "\n",
              "The host and Mike discuss the benefit of asking other AI models for help when Cursor gets stuck. Mike shares his strategy of providing the bug, the attempted solutions, and the expected outcome to another AI model, resulting in improved results. They also touch on the value of explaining code and teaching concepts using AI, as well as adding comments to code for better understanding.\n",
              "\n",
              "Mike highlights the importance of duplicating existing functionality when making similar changes and providing context to AI models. He suggests using templates or starter kits that include boilerplate code for common features like authentication and database integration, emphasizing that Cursor and other AI platforms will likely provide more templates in the future. He offers his own free starter kit as an example.\n",
              "\n",
              "The interview concludes with a discussion about building social media apps using Cursor and the host's previous video on the topic. They encourage viewers to use the comment section for questions, discussions, and sharing their own experiences with Cursor and AI development.\n",
              "\n",
              "## Conclusions\n",
              "\n",
              "The interview offers valuable insights and strategies for effectively using Cursor AI and similar tools. Planning, visualizing, and providing context are emphasized as key factors for successful outcomes. The host and Mike discuss the benefits of using V0 for initial planning and visualization, cursor.directory for providing technology-specific prompts, and tagging documentation for up-to-date information. Asking other AI models for help when Cursor gets stuck, explaining code, and using templates or starter kits are also highlighted as useful strategies.\n",
              "\n",
              "## Summary\n",
              "\n",
              "The host interviews Mike, a front-end developer, to gain insights into best practices for using Cursor AI. Mike emphasizes planning, visualization, and providing context to AI models. He introduces V0 for initial planning and cursor.directory for technology-specific prompts. Tagging documentation and asking other AI models for help when stuck are recommended strategies. Explaining code, adding comments, and using templates are also discussed. Mike offers his own free starter kit as an example. The interview concludes with a discussion about building social media apps and engaging viewers through the comment section."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import cohere\n",
        "from google.colab import userdata\n",
        "co = cohere.Client(api_key=userdata.get('COHERE_API_KEY'))\n",
        "query = f\"\"\"\n",
        "Write a thorough summary for this text: '''\n",
        "{text_ifw[\"text\"]}\n",
        "'''\n",
        "Give extensive info about the content.\n",
        "Discuss all important points raised throughout.\n",
        "At the end, create a section of \"Conclusions\" and another for \"Summary\"\n",
        "\"\"\"\n",
        "\n",
        "cohere_query = co.chat(\n",
        "  model=\"command-r-plus\",\n",
        "  message=query\n",
        ")\n",
        "\n",
        "from IPython.display import Markdown\n",
        "display(Markdown(cohere_query.text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRr7SMrZfTZ2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOtl5mgQklbme0qf4q+pK/O",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
